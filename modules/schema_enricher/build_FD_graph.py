#!/usr/bin/env python3
"""
export_samples_graphs.py
────────────────────────
• Reads mats.<split>_samples from MongoDB (BIRD dataset)
• Reads spider_<split>_samples from MongoDB (Spider dataset)
• Reads spider2_lite_samples from MongoDB (Spider 2.0 dataset)
• Builds a NetworkX graph for each sample – every column-node now has
  • meaning       = column_meaning[<col>]  (pretty name / description)
  • value_desc    = column_value_desc[<col>]    (new)
  • meaning_full  = "<meaning> ; value_desc <...>"  (concatenated)
• Pickles   data/<dataset>_<split>_samples_graph.pkl
• Saves     figures/<dataset>_<split>/<db_id>.png   (schema picture, 1 per DB)
"""
from __future__ import annotations
import argparse, os, pickle, random
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

import networkx as nx
import matplotlib.pyplot as plt
from pymongo import MongoClient
from tqdm import tqdm


# ────────────────── configuration ──────────────────
MONGO_URI = "mongodb://192.168.1.108:27017"
DB_NAME   = "mats"
DATA_DIR  = Path("data")          # pickles
FIG_DIR   = Path("figures")       # PNG graphs


# ────────────────── graph builder ──────────────────
def build_graph(doc: Dict[str, Any]) -> nx.DiGraph:
	"""Create a schema graph from one Mongo document."""
	G = nx.DiGraph()
	
	col_info        = doc.get("column_info", {})
	col_meaning     = doc.get("column_meaning", {})        # (already merged)
	col_value_desc  = doc.get("column_value_desc", {})     # NEW
	table_meaning   = doc.get("table_meaning", {})
	# Fallback meanings generated by automation
	gen_col_meaning = doc.get("generated_column_meaning", {}) or {}
	foreign_keys    = doc.get("foreign_keys", {})
	primary_keys    = doc.get("primary_keys", {})
	# Merge generated keys into curated keys (union) for Spider2 and others
	gen_fk = doc.get("generated_foreign_keys", {}) or {}
	gen_pk = doc.get("generated_primary_keys", {}) or {}
	# Merge PKs: {table: sorted unique cols}
	if gen_pk:
		merged_pk: Dict[str, List[str]] = {}
		for tbl, cols in (primary_keys or {}).items():
			merged_pk.setdefault(tbl, [])
			for c in cols or []:
				if c not in merged_pk[tbl]:
					merged_pk[tbl].append(c)
		for tbl, cols in gen_pk.items():
			merged_pk.setdefault(tbl, [])
			for c in cols or []:
				if c not in merged_pk[tbl]:
					merged_pk[tbl].append(c)
		for tbl in list(merged_pk.keys()):
			merged_pk[tbl] = sorted(merged_pk[tbl])
		primary_keys = merged_pk
	# Merge FKs: {table: list of fk objects}, dedup by (from,to)
	if gen_fk:
		merged_fk: Dict[str, List[Dict[str, Any]]] = {}
		def add_fk(tbl: str, fk_obj: Dict[str, Any]) -> None:
			if not fk_obj:
				return
			fr = fk_obj.get("from", "")
			to = fk_obj.get("to", "")
			ref_table = fk_obj.get("ref_table", "")
			if not fr or not to or not ref_table:
				return
			merged_fk.setdefault(tbl, [])
			# Dedup by (from,to)
			seen = {(f.get("from", ""), f.get("to", "")) for f in merged_fk[tbl]}
			if (fr, to) not in seen:
				merged_fk[tbl].append({
					"from": fr,
					"to": to,
					"ref_table": ref_table,
					"on_update": fk_obj.get("on_update", "NO ACTION"),
					"on_delete": fk_obj.get("on_delete", "NO ACTION"),
					"match": fk_obj.get("match", "SIMPLE"),
				})
		# Seed with curated FKs
		for tbl, fks in (foreign_keys or {}).items():
			for fk in fks or []:
				add_fk(tbl, fk)
		# Add generated FKs
		for tbl, fks in gen_fk.items():
			for fk in fks or []:
				add_fk(tbl, fk)
		# Sort for stability
		for tbl in list(merged_fk.keys()):
			merged_fk[tbl] = sorted(merged_fk[tbl], key=lambda x: (x.get("from", ""), x.get("to", "")))
		foreign_keys = merged_fk
	
	# Track primary keys and foreign key columns
	primary_key_cols = set()
	foreign_key_cols = set()
	
	# Collect primary key columns
	for table, pk_cols in primary_keys.items():
		for pk in pk_cols:
			primary_key_cols.add(f"{table}.{pk}")
	
	# Collect foreign key columns
	for src_table, fks in foreign_keys.items():
		for fk in fks:
			foreign_key_cols.add(f"{src_table}.{fk['from']}")
			foreign_key_cols.add(f"{fk['ref_table']}.{fk['to']}")
	
	# 1) nodes ----------------------------------------------------
	for col_name, info in col_info.items():
		table, _ = col_name.split(".", 1)
		
		meaning_txt   = col_meaning.get(col_name, "").strip()
		if not meaning_txt:
			# Use generated meaning if curated meaning is missing/blank
			meaning_txt = gen_col_meaning.get(col_name, "").strip()
		value_txt     = col_value_desc.get(col_name, "").strip()
		
		# meaning_full  = meaning_txt
		# if value_txt:
		#     if meaning_full:
		#         meaning_full += " ; "
		#     meaning_full += f"value_desc {value_txt}"
		
		# Truncate similar_values to 10 words if too long
		similar_values = info.get("similar_values", [])
		truncated_similar_values = []
		for value in similar_values:
			if isinstance(value, str):
				words = value.split()
				if len(words) > 5:
					truncated_value = " ".join(words[:5]) + "..."
					truncated_similar_values.append(truncated_value)
				else:
					truncated_similar_values.append(value)
			else:
				truncated_similar_values.append(value)
		truncated_similar_values = [f'"{value}"' for value in truncated_similar_values]
		
		G.add_node(
			col_name,
			node_name      = col_name,
			type           = info.get("type", ""),
			meaning        = meaning_txt,
			value_desc     = value_txt,
			# meaning_full   = meaning_full,
			similar_values = truncated_similar_values,
			has_null       = bool(info.get("has_null", False)),
			table_meaning  = table_meaning.get(table, table),
			is_primary_key = col_name in primary_key_cols,
			is_in_foreign_key = col_name in foreign_key_cols,
		)
	
	# helper map {table → [columns …]}
	tbl_cols: Dict[str, List[str]] = {}
	for col in G.nodes:
		tbl_cols.setdefault(col.split(".", 1)[0], []).append(col)
	
	# 2) foreign-key edges ---------------------------------------
	for src_table, fks in foreign_keys.items():
		for fk in fks:
			src = f"{src_table}.{fk['from']}"
			tgt = f"{fk['ref_table']}.{fk['to']}"
			if src not in G or tgt not in G:
				continue
			
			G.add_edge(src, tgt, edge_type="foreign_key")
			
			# every column in table(src) → src
			for col in tbl_cols.get(src_table, []):
				if col != src and not G.has_edge(col, src):
					G.add_edge(col, src, edge_type="col_to_foreign_key")
			
			# every column in table(tgt) → tgt
			ref_table = fk["ref_table"]
			for col in tbl_cols.get(ref_table, []):
				if col != tgt and not G.has_edge(col, tgt):
					G.add_edge(col, tgt, edge_type="col_to_foreign_key")
	
	# 3) primary-key hubs ---------------------------------------
	for table, pk_cols in primary_keys.items():
		for pk in pk_cols:
			pk_full = f"{table}.{pk}"
			if pk_full not in G:
				continue
			for col in tbl_cols.get(table, []):
				if col != pk_full and not G.has_edge(col, pk_full):
					G.add_edge(col, pk_full, edge_type="col_to_primary_key")
	
	return G


# ────────────────── drawing util ───────────────────
def save_graph_png(G: nx.DiGraph, out_path: Path) -> None:
    """Draw graph *G* and save to *out_path*."""
    pos = nx.spring_layout(G, seed=42)
    plt.figure(figsize=(10, 8))

    nx.draw_networkx_nodes(G, pos, node_size=400, node_color="#a6cee3")
    edge_colors = [
        "red" if d.get("edge_type") == "foreign_key" else "grey"
        for _, _, d in G.edges(data=True)
    ]
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, arrows=True, width=1.4)  # type: ignore
    nx.draw_networkx_labels(G, pos, font_size=7)

    plt.axis("off")
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()


# ────────────────── helpers for Spider2 keys ─────────────────
def merge_spider2_generated_keys(doc: Dict[str, Any], fallback_coll) -> None:
    """Merge generated keys from original doc and fallback collection into doc.
    - Unions generated_primary_keys per table
    - Dedups generated_foreign_keys per (from, to) per table
    Modifies doc in place.
    """
    # Merge generated_primary_keys
    base_pk = doc.get("generated_primary_keys", {}) or {}
    fb_pk: Dict[str, List[str]] = {}
    if fallback_coll is not None:
        try:
            fb_doc = fallback_coll.find_one({"_id": doc["_id"]}, {"generated_primary_keys": 1, "generated_foreign_keys": 1})
            if fb_doc and isinstance(fb_doc.get("generated_primary_keys"), dict):
                fb_pk = fb_doc.get("generated_primary_keys", {}) or {}
            fb_fk_doc = fb_doc if fb_doc else None
        except Exception:
            fb_fk_doc = None
    else:
        fb_fk_doc = None

    merged_pk: Dict[str, List[str]] = {}
    for tbl, cols in (base_pk or {}).items():
        merged_pk.setdefault(tbl, [])
        for c in cols or []:
            if c not in merged_pk[tbl]:
                merged_pk[tbl].append(c)
    for tbl, cols in (fb_pk or {}).items():
        merged_pk.setdefault(tbl, [])
        for c in cols or []:
            if c not in merged_pk[tbl]:
                merged_pk[tbl].append(c)
    for tbl in list(merged_pk.keys()):
        merged_pk[tbl] = sorted(merged_pk[tbl])
    if merged_pk:
        doc["generated_primary_keys"] = merged_pk

    # Merge generated_foreign_keys
    base_fk = doc.get("generated_foreign_keys", {}) or {}
    fb_fk = {}
    if fb_fk_doc and isinstance(fb_fk_doc.get("generated_foreign_keys"), dict):
        fb_fk = fb_fk_doc.get("generated_foreign_keys", {}) or {}

    if base_fk or fb_fk:
        merged_fk: Dict[str, List[Dict[str, Any]]] = {}

        def add_fk(tbl: str, fk_obj: Dict[str, Any]) -> None:
            if not fk_obj:
                return
            fr = fk_obj.get("from", "")
            to = fk_obj.get("to", "")
            ref_table = fk_obj.get("ref_table", "")
            if not fr or not to or not ref_table:
                return
            merged_fk.setdefault(tbl, [])
            seen = {(f.get("from", ""), f.get("to", "")) for f in merged_fk[tbl]}
            if (fr, to) not in seen:
                merged_fk[tbl].append({
                    "from": fr,
                    "to": to,
                    "ref_table": ref_table,
                    "on_update": fk_obj.get("on_update", "NO ACTION"),
                    "on_delete": fk_obj.get("on_delete", "NO ACTION"),
                    "match": fk_obj.get("match", "SIMPLE"),
                })

        for tbl, fks in (base_fk or {}).items():
            for fk in fks or []:
                add_fk(tbl, fk)
        for tbl, fks in (fb_fk or {}).items():
            for fk in fks or []:
                add_fk(tbl, fk)
        for tbl in list(merged_fk.keys()):
            merged_fk[tbl] = sorted(merged_fk[tbl], key=lambda x: (x.get("from", ""), x.get("to", "")))
        doc["generated_foreign_keys"] = merged_fk


# ────────────────── export routine ─────────────────
def main(split: str, dataset: str, use_merged: bool = False, sample_ids: Optional[List[int]] = None, suffix: str = "") -> None:
    # Determine collection name based on dataset
    if dataset.lower() == "bird":
        collection = "train_samples" if split == "train" else "dev_samples"
    elif dataset.lower() == "spider":
        collection = "spider_train_samples" if split == "train" else "spider_dev_samples"
    elif dataset.lower() == "spider2":
        collection = "spider2_lite_samples"  # Spider 2.0 uses a single collection
        if split != "dev":  # Spider 2.0 only has dev split
            print(f"Warning: Spider 2.0 only has dev split, ignoring split '{split}'")
    else:
        raise ValueError(f"Unknown dataset: {dataset}. Must be 'bird', 'spider', or 'spider2'")
    
    # Build filename suffix
    parts = []
    if use_merged:
        parts.append("merged")
    if sample_ids is not None:
        parts.append("samples")
        if suffix:
            parts.append(suffix)
    
    final_suffix = "_" + "_".join(parts) if parts else ""
    
    # For Spider 2.0, create two versions: with and without evidence
    if dataset.lower() == "spider2":
        pkl_path_with_evidence = DATA_DIR / f"{dataset}_{split}_samples_graph_with_evidence{final_suffix}.pkl"
        pkl_path_no_evidence = DATA_DIR / f"{dataset}_{split}_samples_graph_no_evidence{final_suffix}.pkl"
    else:
        pkl_path = DATA_DIR / f"{dataset}_{split}_samples_graph{final_suffix}.pkl"
    
    split_figs = FIG_DIR / f"{dataset}_{split}"

    DATA_DIR.mkdir(exist_ok=True)
    split_figs.mkdir(parents=True, exist_ok=True)

    print(f"Connecting to MongoDB  →  {MONGO_URI}")
    print(f"Dataset: {dataset.upper()}, Split: {split}, Collection: {collection}")
    print(f"Using {'merged' if use_merged else 'base'} columns for gold labels")
    
    with MongoClient(MONGO_URI) as client:
        coll = client[DB_NAME][collection]
        # Read fallback generated keys for Spider 2.0 samples when present
        fallback_coll = None
        if dataset.lower() == "spider2":
            fallback_coll = client[DB_NAME].get_collection("spider2_lite_samples_2")
        
        # Build query filter for specific sample IDs if provided
        query_filter = {}
        if sample_ids is not None:
            query_filter["_id"] = {"$in": sample_ids}
            print(f"Filtering to specific sample IDs: {sample_ids}")
        
        cursor = coll.find(query_filter, no_cursor_timeout=True)

        triples: List[Tuple[str, nx.DiGraph, List[str], str]] = []
        triples_with_evidence: List[Tuple[str, nx.DiGraph, List[str], str]] = []
        triples_no_evidence: List[Tuple[str, nx.DiGraph, List[str], str]] = []
        drawn_db_ids: set[str] = set()
        wrong_samples_filtered = 0

        for doc in tqdm(cursor, desc=f"Processing {dataset} {split} samples"):
            # Filter out wrong samples for Spider 2.0
            if dataset.lower() == "spider2" and doc.get("wrong_label", False):
                wrong_samples_filtered += 1
                continue
                
            question_with_evidence = f"{doc.get('question','')}\nExternal knowledge: {doc.get('evidence','')}"
            question_no_evidence = f"{doc.get('question','')}\nExternal knowledge: "
            
            # Choose which columns to use based on use_merged flag
            if use_merged:
                gold = doc.get("merged", {}).get("used_columns", [])
                if not gold:  # Fallback to base if merged not available
                    gold = doc.get("used_columns", [])
                    print(f"Warning: Document {doc['_id']} has no merged.used_columns, falling back to base")
            else:
                gold = doc.get("used_columns", [])
            
            # If Spider 2.0, attempt to merge generated keys from fallback collection via helper
            if dataset.lower() == "spider2":
                merge_spider2_generated_keys(doc, fallback_coll)
 
            G = build_graph(doc)
            sample_id = doc['_id']
            
            # For Spider 2.0, create two versions
            if dataset.lower() == "spider2":
                triples_with_evidence.append((question_with_evidence, G, gold, sample_id))
                triples_no_evidence.append((question_no_evidence, G, gold, sample_id))
            else:
                triples.append((question_with_evidence, G, gold, sample_id))

            # ---------- draw one PNG per sample/db ----------
            if dataset.lower() == "spider2":
                schema_list = doc.get("schema", []) or []
                db_id = doc.get("db_id")
                if db_id and db_id not in drawn_db_ids and isinstance(schema_list, list) and len(schema_list) < 100:
                    png_path = split_figs / f"{db_id}.png"
                    if not png_path.exists():           # avoid re-drawing across runs
                        save_graph_png(G, png_path)
                    drawn_db_ids.add(db_id)
            else:
                db_id = doc.get("db_id")
                if db_id and db_id not in drawn_db_ids:
                    png_path = split_figs / f"{db_id}.png"
                    if not png_path.exists():           # avoid re-drawing across runs
                        save_graph_png(G, png_path)
                    drawn_db_ids.add(db_id)

        cursor.close()

    # ---------- write pickle ----------
    if dataset.lower() == "spider2":
        # Save both versions for Spider 2.0
        with open(pkl_path_with_evidence, "wb") as f:
            pickle.dump(triples_with_evidence, f)
        with open(pkl_path_no_evidence, "wb") as f:
            pickle.dump(triples_no_evidence, f)
        print(f"\n✓ Saved {len(triples_with_evidence)} triples with evidence → {pkl_path_with_evidence.resolve()}")
        print(f"✓ Saved {len(triples_no_evidence)} triples without evidence → {pkl_path_no_evidence.resolve()}")
        if wrong_samples_filtered > 0:
            print(f"✓ Filtered out {wrong_samples_filtered} samples with wrong_label=True")
    else:
        # Save single version for other datasets
        with open(pkl_path, "wb") as f:
            pickle.dump(triples, f)
        print(f"\n✓ Saved {len(triples)} triples → {pkl_path.resolve()}")
    
    if dataset.lower() != "spider2":
        print(f"✓ Saved {len(drawn_db_ids)} schema PNGs → {split_figs.resolve()}")
    else:
        print("✓ Skipped PNG generation for Spider 2.0")

    # ---------- random sanity-check ----------
    if dataset.lower() == "spider2":
        q_with, G_sample, gold_cols, _ = random.choice(triples_with_evidence)
        q_without, _, _, _ = random.choice(triples_no_evidence)
        print("\nRandom sample question WITH evidence:\n", q_with)
        print("\nRandom sample question WITHOUT evidence:\n", q_without)
        print("True columns:", gold_cols)
    else:
        q, G_sample, gold_cols, _ = random.choice(triples)
        print("\nRandom sample question:\n", q)
        print("True columns:", gold_cols)


# ───────────────────────── CLI ─────────────────────────
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Export (question, graph, true_cols) triples and draw schema graphs."
    )
    parser.add_argument("--split", choices=["train", "dev"], default="train",
                        help="Dataset split to export (default: train, Spider 2.0 only supports dev)")
    parser.add_argument("--dataset", choices=["bird", "spider", "spider2"], default="bird",
                        help="Dataset to use: BIRD, Spider, or Spider2.0 (default: bird)")
    parser.add_argument("--use-merged", action="store_true",
                        help="Use merged.used_columns (from augmented SQL queries) instead of base used_columns")
    parser.add_argument("--sample_ids", nargs="+", type=int, default=None,
                        help="Specific sa`mple IDs to process (default: all samples)")
    parser.add_argument("--suffix", type=str, default="",
                        help="Suffix to add to output filenames")
    args = parser.parse_args()

    main(args.split, args.dataset, args.use_merged, args.sample_ids, args.suffix)
